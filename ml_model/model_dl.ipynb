{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/technical/pupil-detection/pupil_api/my-test-env/lib/python3.9/site-packages/h5py/__init__.py:36: UserWarning: h5py is running against HDF5 1.12.2 when it was built against 1.12.1, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from os import walk\n",
    "import json\n",
    "from statsmodels import robust\n",
    "\n",
    "# Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Conv1D,MaxPooling1D,Flatten\n",
    "# Train-Test\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Read file from folder"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "['/Users/[USER_NAME]/Downloads/data/123802.json',\n '/Users/[USER_NAME]/Downloads/data/119509.json',\n '/Users/[USER_NAME]/Downloads/data/124352.json',\n '/Users/[USER_NAME]/Downloads/data/123995.json',\n '/Users/[USER_NAME]/Downloads/data/200633.json',\n '/Users/[USER_NAME]/Downloads/data/120546.json',\n '/Users/[USER_NAME]/Downloads/data/124773.json',\n '/Users/[USER_NAME]/Downloads/data/118170.json',\n '/Users/[USER_NAME]/Downloads/data/124731.json',\n '/Users/[USER_NAME]/Downloads/data/124727.json',\n '/Users/[USER_NAME]/Downloads/data/124737.json',\n '/Users/[USER_NAME]/Downloads/data/114456.json',\n '/Users/[USER_NAME]/Downloads/data/121922.json',\n '/Users/[USER_NAME]/Downloads/data/119414.json',\n '/Users/[USER_NAME]/Downloads/data/119506.json',\n '/Users/[USER_NAME]/Downloads/data/124311.json']"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "file_list=[]\n",
    "for dirpath,dirnames,filepath in walk('/Users/[USER_NAME]/Downloads/data'):\n",
    "    for file in filepath:\n",
    "        path=dirpath+'/'+file\n",
    "        file_list.append(path)\n",
    "\n",
    "file_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# speedy outlier threshold calculation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def get_threshold(d_list):\n",
    "    d_base=[]\n",
    "    for i,c in enumerate(d_list):\n",
    "        if i==0:\n",
    "            d_base.append(max(abs(d_list[i+1]-d_list[i]),0))\n",
    "        elif i==len(d_list)-1:\n",
    "            d_base.append(max(abs(d_list[i]-d_list[i-1]),0))\n",
    "        else:\n",
    "            d_base.append(max(abs(d_list[i+1]-d_list[i]),abs(d_list[i]-d_list[i-1])))\n",
    "\n",
    "    median_d_base=np.median(d_base)\n",
    "    d_base_new=[abs(x-median_d_base) for x in d_base]\n",
    "\n",
    "    mad_base_old=robust.mad(np.array(d_base),c=1)\n",
    "    mad_base=np.median(d_base_new)\n",
    "    threshold_max = np.median(d_list)+40*mad_base\n",
    "    threshold_min = np.median(d_list)-40*mad_base\n",
    "    return threshold_min,threshold_max"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# cleaning data for dataframe generation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file :  /Users/[USER_NAME]/Downloads/data/123802.json\n",
      "reading file :  /Users/[USER_NAME]/Downloads/data/119509.json\n",
      "reading file :  /Users/[USER_NAME]/Downloads/data/124352.json\n",
      "reading file :  /Users/[USER_NAME]/Downloads/data/123995.json\n",
      "reading file :  /Users/[USER_NAME]/Downloads/data/200633.json\n",
      "reading file :  /Users/[USER_NAME]/Downloads/data/120546.json\n",
      "reading file :  /Users/[USER_NAME]/Downloads/data/124773.json\n",
      "reading file :  /Users/[USER_NAME]/Downloads/data/118170.json\n",
      "reading file :  /Users/[USER_NAME]/Downloads/data/124731.json\n",
      "reading file :  /Users/[USER_NAME]/Downloads/data/124727.json\n",
      "reading file :  /Users/[USER_NAME]/Downloads/data/124737.json\n",
      "reading file :  /Users/[USER_NAME]/Downloads/data/114456.json\n",
      "reading file :  /Users/[USER_NAME]/Downloads/data/121922.json\n",
      "reading file :  /Users/[USER_NAME]/Downloads/data/119414.json\n",
      "reading file :  /Users/[USER_NAME]/Downloads/data/119506.json\n",
      "reading file :  /Users/[USER_NAME]/Downloads/data/124311.json\n"
     ]
    }
   ],
   "source": [
    "t_list=[]\n",
    "for file_path in file_list:\n",
    "    print('reading file : ',file_path)\n",
    "    participant_id=file_path.split('/')[-1:][0].split('.')[0]\n",
    "    f=open(file_path)\n",
    "    json_data=json.load(f)\n",
    "\n",
    "    for trail in json_data.get('trials'):\n",
    "        tmp_list=[]\n",
    "        rating=trail.get('rating')\n",
    "        diameter_list=trail.get('pupil_dilation')\n",
    "        # print(diameter_list)\n",
    "        # artifact outlier detection start\n",
    "        if 0 in diameter_list:\n",
    "            # print('0 present diameter : ')\n",
    "            diameter_indices = [i for i, x in enumerate(diameter_list) if x == 0]\n",
    "            # print(diameter_indices)\n",
    "            for index in diameter_indices:\n",
    "\n",
    "                st_index = 0 if (index-5)<0 else (index-5)\n",
    "                diameter_list[st_index:index]=[0 for x in range(index-st_index)]\n",
    "                end_index = index+5 if (index+5)<len(diameter_list) else len(diameter_list)\n",
    "                diameter_list[index:end_index]=[0 for x in range(end_index-index)]\n",
    "\n",
    "        # artifact outlier detection end\n",
    "\n",
    "        #speedy outlier detection start\n",
    "\n",
    "        threshold_diameter_min,threshold_diameter_max  = get_threshold(diameter_list)\n",
    "        diameter_list = [0 if x>threshold_diameter_max else x for x in diameter_list ]\n",
    "        diameter_list = [0 if x<threshold_diameter_min else x for x in diameter_list ]\n",
    "\n",
    "        # print('0 count: ',diameter_list.count(0))\n",
    "        # print('before: ',diameter_list, ' len: ',len(diameter_list))\n",
    "\n",
    "        dt_diameter=pd.DataFrame(diameter_list,columns=['x'])\n",
    "        dt_diameter['x']=dt_diameter['x'].replace(0,np.nan)\n",
    "        dt_diameter=dt_diameter.interpolate('index',limit_direction='both')\n",
    "\n",
    "        #speedy outlier detection end\n",
    "        #save data to list for generating dataframe\n",
    "        diameter_list = dt_diameter['x'].tolist()\n",
    "        diameter_list=diameter_list[0:296]\n",
    "        diameter_list.append(rating)\n",
    "        # tmp_list.append(diameter_list)\n",
    "        t_list.append(diameter_list)\n",
    "        # print('after : ',diameter_list)\n",
    "    f.close()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "df=pd.DataFrame(t_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "df['rating']=df[296]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "0      3.0\n1      4.0\n2      0.0\n3      3.0\n4      3.0\n      ... \n475    1.0\n476    3.0\n477    3.0\n478    4.0\n479    3.0\nName: 296, Length: 480, dtype: float64"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.pop(296)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "            0         1         2         3         4         5         6  \\\n0    3.216797  3.216797  3.216797  3.216797  3.216797  3.216797  3.216797   \n1    3.470810  3.470810  3.471359  3.459839  3.457794  3.467407  3.464661   \n2    3.403854  3.405106  3.406250  3.400970  3.413254  3.421799  3.410629   \n3    3.737244  3.737244  3.733856  3.743835  3.743332  3.742188  3.741837   \n4    3.293930  3.300217  3.301544  3.304016  3.309082  3.316055  3.329453   \n..        ...       ...       ...       ...       ...       ...       ...   \n475  3.646790  3.650696  3.645737  3.651306  3.658722  3.650162  3.639297   \n476  3.546082  3.522751  3.506439  3.495499  3.490326  3.484406  3.470108   \n477  3.638428  3.641861  3.642761  3.636490  3.628525  3.632095  3.635986   \n478  3.480133  3.480133  3.480133  3.480133  3.480133  3.480133  3.482941   \n479  3.532059  3.532059  3.519562  3.527222  3.525009  3.521255  3.535904   \n\n            7         8         9  ...       287       288       289  \\\n0    3.216797  3.216797  3.216797  ...  3.316177  3.317657  3.310822   \n1    3.459854  3.453629  3.452072  ...  3.441101  3.442383  3.447678   \n2    3.402542  3.394821  3.384430  ...  3.481216  3.479828  3.469696   \n3    3.743469  3.746597  3.744690  ...  3.576734  3.577300  3.577865   \n4    3.331055  3.325867  3.321960  ...  3.511581  3.515945  3.517441   \n..        ...       ...       ...  ...       ...       ...       ...   \n475  3.642319  3.643173  3.644501  ...  3.826083  3.828249  3.830415   \n476  3.460754  3.457748  3.447495  ...  3.825790  3.825790  3.825790   \n477  3.631821  3.631378  3.635910  ...  3.744934  3.746774  3.748613   \n478  3.493301  3.496399  3.502731  ...  3.817566  3.804489  3.778717   \n479  3.544571  3.556488  3.556488  ...  3.646408  3.584473  3.573456   \n\n          290       291       292       293       294       295  rating  \n0    3.301193  3.303116  3.304520  3.313171  3.316513  3.312958     3.0  \n1    3.446960  3.437622  3.431137  3.422394  3.420547  3.423767     4.0  \n2    3.469711  3.468948  3.467346  3.464188  3.462677  3.471527     0.0  \n3    3.578430  3.568329  3.566010  3.568863  3.546310  3.546860     3.0  \n4    3.521683  3.513153  3.523087  3.533752  3.544464  3.551071     3.0  \n..        ...       ...       ...       ...       ...       ...     ...  \n475  3.832581  3.834747  3.879456  3.916977  3.932816  3.936722     1.0  \n476  3.825790  3.825790  3.825790  3.825790  3.825790  3.825790     3.0  \n477  3.750453  3.752292  3.754132  3.755971  3.757811  3.759650     3.0  \n478  3.763626  3.754562  3.744064  3.727539  3.722961  3.713455     4.0  \n479  3.569748  3.549225  3.541962  3.531601  3.529419  3.526993     3.0  \n\n[480 rows x 297 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>287</th>\n      <th>288</th>\n      <th>289</th>\n      <th>290</th>\n      <th>291</th>\n      <th>292</th>\n      <th>293</th>\n      <th>294</th>\n      <th>295</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3.216797</td>\n      <td>3.216797</td>\n      <td>3.216797</td>\n      <td>3.216797</td>\n      <td>3.216797</td>\n      <td>3.216797</td>\n      <td>3.216797</td>\n      <td>3.216797</td>\n      <td>3.216797</td>\n      <td>3.216797</td>\n      <td>...</td>\n      <td>3.316177</td>\n      <td>3.317657</td>\n      <td>3.310822</td>\n      <td>3.301193</td>\n      <td>3.303116</td>\n      <td>3.304520</td>\n      <td>3.313171</td>\n      <td>3.316513</td>\n      <td>3.312958</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3.470810</td>\n      <td>3.470810</td>\n      <td>3.471359</td>\n      <td>3.459839</td>\n      <td>3.457794</td>\n      <td>3.467407</td>\n      <td>3.464661</td>\n      <td>3.459854</td>\n      <td>3.453629</td>\n      <td>3.452072</td>\n      <td>...</td>\n      <td>3.441101</td>\n      <td>3.442383</td>\n      <td>3.447678</td>\n      <td>3.446960</td>\n      <td>3.437622</td>\n      <td>3.431137</td>\n      <td>3.422394</td>\n      <td>3.420547</td>\n      <td>3.423767</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3.403854</td>\n      <td>3.405106</td>\n      <td>3.406250</td>\n      <td>3.400970</td>\n      <td>3.413254</td>\n      <td>3.421799</td>\n      <td>3.410629</td>\n      <td>3.402542</td>\n      <td>3.394821</td>\n      <td>3.384430</td>\n      <td>...</td>\n      <td>3.481216</td>\n      <td>3.479828</td>\n      <td>3.469696</td>\n      <td>3.469711</td>\n      <td>3.468948</td>\n      <td>3.467346</td>\n      <td>3.464188</td>\n      <td>3.462677</td>\n      <td>3.471527</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3.737244</td>\n      <td>3.737244</td>\n      <td>3.733856</td>\n      <td>3.743835</td>\n      <td>3.743332</td>\n      <td>3.742188</td>\n      <td>3.741837</td>\n      <td>3.743469</td>\n      <td>3.746597</td>\n      <td>3.744690</td>\n      <td>...</td>\n      <td>3.576734</td>\n      <td>3.577300</td>\n      <td>3.577865</td>\n      <td>3.578430</td>\n      <td>3.568329</td>\n      <td>3.566010</td>\n      <td>3.568863</td>\n      <td>3.546310</td>\n      <td>3.546860</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3.293930</td>\n      <td>3.300217</td>\n      <td>3.301544</td>\n      <td>3.304016</td>\n      <td>3.309082</td>\n      <td>3.316055</td>\n      <td>3.329453</td>\n      <td>3.331055</td>\n      <td>3.325867</td>\n      <td>3.321960</td>\n      <td>...</td>\n      <td>3.511581</td>\n      <td>3.515945</td>\n      <td>3.517441</td>\n      <td>3.521683</td>\n      <td>3.513153</td>\n      <td>3.523087</td>\n      <td>3.533752</td>\n      <td>3.544464</td>\n      <td>3.551071</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>475</th>\n      <td>3.646790</td>\n      <td>3.650696</td>\n      <td>3.645737</td>\n      <td>3.651306</td>\n      <td>3.658722</td>\n      <td>3.650162</td>\n      <td>3.639297</td>\n      <td>3.642319</td>\n      <td>3.643173</td>\n      <td>3.644501</td>\n      <td>...</td>\n      <td>3.826083</td>\n      <td>3.828249</td>\n      <td>3.830415</td>\n      <td>3.832581</td>\n      <td>3.834747</td>\n      <td>3.879456</td>\n      <td>3.916977</td>\n      <td>3.932816</td>\n      <td>3.936722</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>476</th>\n      <td>3.546082</td>\n      <td>3.522751</td>\n      <td>3.506439</td>\n      <td>3.495499</td>\n      <td>3.490326</td>\n      <td>3.484406</td>\n      <td>3.470108</td>\n      <td>3.460754</td>\n      <td>3.457748</td>\n      <td>3.447495</td>\n      <td>...</td>\n      <td>3.825790</td>\n      <td>3.825790</td>\n      <td>3.825790</td>\n      <td>3.825790</td>\n      <td>3.825790</td>\n      <td>3.825790</td>\n      <td>3.825790</td>\n      <td>3.825790</td>\n      <td>3.825790</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>477</th>\n      <td>3.638428</td>\n      <td>3.641861</td>\n      <td>3.642761</td>\n      <td>3.636490</td>\n      <td>3.628525</td>\n      <td>3.632095</td>\n      <td>3.635986</td>\n      <td>3.631821</td>\n      <td>3.631378</td>\n      <td>3.635910</td>\n      <td>...</td>\n      <td>3.744934</td>\n      <td>3.746774</td>\n      <td>3.748613</td>\n      <td>3.750453</td>\n      <td>3.752292</td>\n      <td>3.754132</td>\n      <td>3.755971</td>\n      <td>3.757811</td>\n      <td>3.759650</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>478</th>\n      <td>3.480133</td>\n      <td>3.480133</td>\n      <td>3.480133</td>\n      <td>3.480133</td>\n      <td>3.480133</td>\n      <td>3.480133</td>\n      <td>3.482941</td>\n      <td>3.493301</td>\n      <td>3.496399</td>\n      <td>3.502731</td>\n      <td>...</td>\n      <td>3.817566</td>\n      <td>3.804489</td>\n      <td>3.778717</td>\n      <td>3.763626</td>\n      <td>3.754562</td>\n      <td>3.744064</td>\n      <td>3.727539</td>\n      <td>3.722961</td>\n      <td>3.713455</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>479</th>\n      <td>3.532059</td>\n      <td>3.532059</td>\n      <td>3.519562</td>\n      <td>3.527222</td>\n      <td>3.525009</td>\n      <td>3.521255</td>\n      <td>3.535904</td>\n      <td>3.544571</td>\n      <td>3.556488</td>\n      <td>3.556488</td>\n      <td>...</td>\n      <td>3.646408</td>\n      <td>3.584473</td>\n      <td>3.573456</td>\n      <td>3.569748</td>\n      <td>3.549225</td>\n      <td>3.541962</td>\n      <td>3.531601</td>\n      <td>3.529419</td>\n      <td>3.526993</td>\n      <td>3.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>480 rows × 297 columns</p>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "df['rating']=df['rating'].astype(int)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "df=df[df['rating']!=0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "        0    1    2    3    4    5    6    7    8    9    ...  286  287  288  \\\nrating                                                    ...                  \n1        58   58   58   58   58   58   58   58   58   58  ...   58   58   58   \n2       147  147  147  147  147  147  147  147  147  147  ...  147  147  147   \n3       201  201  201  201  201  201  201  201  201  201  ...  201  201  201   \n4        68   68   68   68   68   68   68   68   68   68  ...   68   68   68   \n\n        289  290  291  292  293  294  295  \nrating                                     \n1        58   58   58   58   58   58   58  \n2       147  147  147  147  147  147  147  \n3       201  201  201  201  201  201  201  \n4        68   68   68   68   68   68   68  \n\n[4 rows x 296 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>286</th>\n      <th>287</th>\n      <th>288</th>\n      <th>289</th>\n      <th>290</th>\n      <th>291</th>\n      <th>292</th>\n      <th>293</th>\n      <th>294</th>\n      <th>295</th>\n    </tr>\n    <tr>\n      <th>rating</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>58</td>\n      <td>58</td>\n      <td>58</td>\n      <td>58</td>\n      <td>58</td>\n      <td>58</td>\n      <td>58</td>\n      <td>58</td>\n      <td>58</td>\n      <td>58</td>\n      <td>...</td>\n      <td>58</td>\n      <td>58</td>\n      <td>58</td>\n      <td>58</td>\n      <td>58</td>\n      <td>58</td>\n      <td>58</td>\n      <td>58</td>\n      <td>58</td>\n      <td>58</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>147</td>\n      <td>147</td>\n      <td>147</td>\n      <td>147</td>\n      <td>147</td>\n      <td>147</td>\n      <td>147</td>\n      <td>147</td>\n      <td>147</td>\n      <td>147</td>\n      <td>...</td>\n      <td>147</td>\n      <td>147</td>\n      <td>147</td>\n      <td>147</td>\n      <td>147</td>\n      <td>147</td>\n      <td>147</td>\n      <td>147</td>\n      <td>147</td>\n      <td>147</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>201</td>\n      <td>201</td>\n      <td>201</td>\n      <td>201</td>\n      <td>201</td>\n      <td>201</td>\n      <td>201</td>\n      <td>201</td>\n      <td>201</td>\n      <td>201</td>\n      <td>...</td>\n      <td>201</td>\n      <td>201</td>\n      <td>201</td>\n      <td>201</td>\n      <td>201</td>\n      <td>201</td>\n      <td>201</td>\n      <td>201</td>\n      <td>201</td>\n      <td>201</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>68</td>\n      <td>68</td>\n      <td>68</td>\n      <td>68</td>\n      <td>68</td>\n      <td>68</td>\n      <td>68</td>\n      <td>68</td>\n      <td>68</td>\n      <td>68</td>\n      <td>...</td>\n      <td>68</td>\n      <td>68</td>\n      <td>68</td>\n      <td>68</td>\n      <td>68</td>\n      <td>68</td>\n      <td>68</td>\n      <td>68</td>\n      <td>68</td>\n      <td>68</td>\n    </tr>\n  </tbody>\n</table>\n<p>4 rows × 296 columns</p>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('rating').count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# converting output type\n",
    "# 0 == dislike  for rating 1,2\n",
    "# 1 == like  for rating 3,4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "df['remarks']=df['rating'].apply(lambda x: 0 if x in [1,2] else 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# generatin input and output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "dataset = df[:295].values\n",
    "df_t = df.drop(\"rating\", axis = 1)\n",
    "x = df_t.drop(\"remarks\", axis = 1)\n",
    "y = df['remarks']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# time series augmentation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from tsaug import  Drift"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "my_aug = (\n",
    "    Drift(max_drift=(0.1, 0.3))\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "x_aug=[]\n",
    "y_aug=[]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "for i in zip(x.values,y.values):\n",
    "    x_aug.append(my_aug.augment(i[0]))\n",
    "    y_aug.append(i[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "df_x_aug=pd.DataFrame(x_aug)\n",
    "df_y_aug=pd.DataFrame(y_aug)\n",
    "x=pd.concat([x,df_x_aug])\n",
    "y=pd.concat([y,df_y_aug])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1896 1896\n"
     ]
    }
   ],
   "source": [
    "print(len(x),len(y))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x.values, y, test_size=0.10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# model generation\n",
    "# CNN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_4 (Conv1D)           (None, 296, 1)            3         \n",
      "                                                                 \n",
      " max_pooling1d_4 (MaxPooling  (None, 148, 1)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_5 (Conv1D)           (None, 148, 3)            12        \n",
      "                                                                 \n",
      " max_pooling1d_5 (MaxPooling  (None, 49, 3)            0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 147)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 15)                2220      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 2)                 32        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,267\n",
      "Trainable params: 2,267\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# model.add(Dense(60, input_shape = (296,), activation = \"relu\"))\n",
    "model.add(Conv1D(filters=1, kernel_size=2, padding='same', activation='relu',\n",
    "                         input_shape=(296,1 )))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=3, kernel_size=3, padding='same', activation='softmax',\n",
    "                         input_shape=(296,1 )))\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "# model.add(Dense(30, activation = \"relu\"))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Dense(15, activation = \"softmax\"))\n",
    "# model.add(Dropout(0.1))\n",
    "model.add(Dense(2, activation = \"softmax\"))\n",
    "# model.add(Dense(units=4, activation='linear'))\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd', metrics=['accuracy'])\n",
    "# model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 2/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.5422\n",
      "Epoch 3/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 4/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.5422\n",
      "Epoch 5/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5434\n",
      "Epoch 6/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5422\n",
      "Epoch 7/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 8/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 9/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5422\n",
      "Epoch 10/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 11/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5434\n",
      "Epoch 12/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 13/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 14/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 15/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 16/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5422\n",
      "Epoch 17/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5422\n",
      "Epoch 18/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 19/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 20/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 21/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 22/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 23/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5422\n",
      "Epoch 24/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5422\n",
      "Epoch 25/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5434\n",
      "Epoch 26/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5434\n",
      "Epoch 27/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5434\n",
      "Epoch 28/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 29/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5416\n",
      "Epoch 30/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5434\n",
      "Epoch 31/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 32/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 33/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 34/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 35/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 36/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 37/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 38/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 39/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5434\n",
      "Epoch 40/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 41/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 42/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 43/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 44/100\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.2500 - accuracy: 0.5416\n",
      "Epoch 45/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.5422\n",
      "Epoch 46/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.5422\n",
      "Epoch 47/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 48/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 49/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 50/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 51/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 52/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5440\n",
      "Epoch 53/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 54/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 55/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 56/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 57/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5440\n",
      "Epoch 58/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5422\n",
      "Epoch 59/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 60/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.5440\n",
      "Epoch 61/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 62/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.5434\n",
      "Epoch 63/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.5434\n",
      "Epoch 64/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.5434\n",
      "Epoch 65/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 66/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 67/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 68/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 69/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 70/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 71/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 72/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 73/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.5434\n",
      "Epoch 74/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5422\n",
      "Epoch 75/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 76/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5434\n",
      "Epoch 77/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 78/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 79/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 80/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 81/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.5422\n",
      "Epoch 82/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5434\n",
      "Epoch 83/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5440\n",
      "Epoch 84/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 85/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5434\n",
      "Epoch 86/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5445\n",
      "Epoch 87/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.5434\n",
      "Epoch 88/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.5434\n",
      "Epoch 89/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 90/100\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 91/100\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.2500 - accuracy: 0.5434\n",
      "Epoch 92/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.5440\n",
      "Epoch 93/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.5434\n",
      "Epoch 94/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.5434\n",
      "Epoch 95/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5422\n",
      "Epoch 96/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 97/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 98/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5428\n",
      "Epoch 99/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5422\n",
      "Epoch 100/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5428\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x28941c850>"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, verbose=1, epochs=100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.5000871 , 0.49991292],\n       [0.5000211 , 0.49997893],\n       [0.49990094, 0.50009906],\n       [0.5000877 , 0.49991235],\n       [0.49988458, 0.5001154 ],\n       [0.49997926, 0.5000208 ],\n       [0.49988753, 0.5001124 ],\n       [0.5000224 , 0.49997765],\n       [0.49995035, 0.50004965],\n       [0.49977154, 0.5002285 ],\n       [0.4998116 , 0.50018835],\n       [0.49991778, 0.50008225],\n       [0.49993014, 0.50006986],\n       [0.50007874, 0.4999212 ],\n       [0.5000541 , 0.49994594],\n       [0.50016546, 0.49983448],\n       [0.49989536, 0.5001046 ],\n       [0.50002795, 0.4999721 ],\n       [0.499888  , 0.50011194],\n       [0.49978533, 0.50021476],\n       [0.49965632, 0.5003437 ],\n       [0.5003259 , 0.49967414],\n       [0.49999374, 0.50000626],\n       [0.4999621 , 0.5000379 ],\n       [0.5000166 , 0.4999834 ],\n       [0.5000676 , 0.4999324 ],\n       [0.5000495 , 0.49995056],\n       [0.50014985, 0.49985012],\n       [0.49984175, 0.5001583 ],\n       [0.500126  , 0.49987403],\n       [0.50009793, 0.499902  ],\n       [0.5000793 , 0.4999207 ],\n       [0.49989554, 0.5001044 ],\n       [0.4998744 , 0.50012565],\n       [0.49991402, 0.50008595],\n       [0.49980536, 0.5001946 ],\n       [0.5000374 , 0.49996263],\n       [0.4999897 , 0.5000104 ],\n       [0.49992895, 0.50007105],\n       [0.5000407 , 0.4999593 ],\n       [0.5000796 , 0.49992046],\n       [0.49982476, 0.5001753 ],\n       [0.49987102, 0.500129  ],\n       [0.50012213, 0.49987793],\n       [0.5000966 , 0.49990335],\n       [0.49998835, 0.5000117 ],\n       [0.5000541 , 0.49994585],\n       [0.49986857, 0.5001315 ],\n       [0.50001216, 0.4999878 ],\n       [0.49996376, 0.5000363 ],\n       [0.499778  , 0.500222  ],\n       [0.5002276 , 0.49977246],\n       [0.50004876, 0.4999513 ],\n       [0.5000804 , 0.4999196 ],\n       [0.4998253 , 0.5001747 ],\n       [0.49978566, 0.5002144 ],\n       [0.4998475 , 0.50015247],\n       [0.49998248, 0.5000176 ],\n       [0.49986976, 0.50013024],\n       [0.50017536, 0.49982467],\n       [0.50024116, 0.49975887],\n       [0.500074  , 0.499926  ],\n       [0.5001226 , 0.49987745],\n       [0.49976093, 0.5002391 ],\n       [0.49979344, 0.5002065 ],\n       [0.50008076, 0.49991918],\n       [0.50012076, 0.49987924],\n       [0.5002616 , 0.49973843],\n       [0.50028574, 0.49971423],\n       [0.5001148 , 0.49988514],\n       [0.49998182, 0.50001824],\n       [0.49990818, 0.5000918 ],\n       [0.5000637 , 0.49993628],\n       [0.50001144, 0.49998862],\n       [0.500029  , 0.49997094],\n       [0.50014085, 0.4998592 ],\n       [0.5000951 , 0.49990484],\n       [0.49985301, 0.50014704],\n       [0.49992856, 0.5000714 ],\n       [0.4994078 , 0.5005922 ],\n       [0.49997935, 0.5000206 ],\n       [0.49997675, 0.5000233 ],\n       [0.49965084, 0.5003492 ],\n       [0.49996147, 0.5000385 ],\n       [0.4999708 , 0.50002927],\n       [0.5000968 , 0.49990317],\n       [0.5001482 , 0.49985182],\n       [0.5001266 , 0.49987346],\n       [0.5000916 , 0.49990836],\n       [0.49994016, 0.5000599 ],\n       [0.49985763, 0.5001424 ],\n       [0.4996652 , 0.50033486],\n       [0.50008214, 0.4999178 ],\n       [0.4999259 , 0.5000741 ],\n       [0.49981776, 0.5001822 ],\n       [0.5000974 , 0.4999026 ],\n       [0.50009227, 0.49990773],\n       [0.5001202 , 0.49987972],\n       [0.50011057, 0.49988943],\n       [0.50002396, 0.49997604],\n       [0.50017846, 0.49982154],\n       [0.500062  , 0.49993804],\n       [0.49986228, 0.50013775],\n       [0.5001055 , 0.49989453],\n       [0.50008017, 0.49991977],\n       [0.4999317 , 0.5000683 ],\n       [0.49992868, 0.5000713 ],\n       [0.5002499 , 0.49975008],\n       [0.5000729 , 0.49992713],\n       [0.50011253, 0.49988744],\n       [0.49996418, 0.5000358 ],\n       [0.50003606, 0.49996394],\n       [0.49985847, 0.50014156],\n       [0.49991018, 0.50008976],\n       [0.4999383 , 0.50006163],\n       [0.49993712, 0.5000629 ],\n       [0.50006604, 0.49993396],\n       [0.50018746, 0.49981248],\n       [0.5001476 , 0.49985248],\n       [0.50005424, 0.49994576],\n       [0.49990112, 0.5000989 ],\n       [0.49960598, 0.500394  ],\n       [0.50000256, 0.4999975 ],\n       [0.49983385, 0.5001661 ],\n       [0.500078  , 0.499922  ],\n       [0.50012076, 0.49987915],\n       [0.49986953, 0.5001305 ],\n       [0.50013876, 0.4998613 ],\n       [0.49998596, 0.500014  ],\n       [0.50010157, 0.4998984 ],\n       [0.500078  , 0.49992192],\n       [0.49979073, 0.5002093 ],\n       [0.50010973, 0.49989018],\n       [0.49994966, 0.5000503 ],\n       [0.5000816 , 0.4999184 ],\n       [0.49964303, 0.500357  ],\n       [0.50014216, 0.4998578 ],\n       [0.50020343, 0.49979657],\n       [0.4999087 , 0.50009125],\n       [0.5002275 , 0.4997725 ],\n       [0.5001476 , 0.4998524 ],\n       [0.49989304, 0.5001069 ],\n       [0.49988747, 0.50011253],\n       [0.5002782 , 0.49972183],\n       [0.49982828, 0.50017166],\n       [0.49991304, 0.50008696],\n       [0.49975413, 0.5002459 ],\n       [0.5000734 , 0.49992657],\n       [0.5001203 , 0.49987972],\n       [0.49991864, 0.50008136],\n       [0.5000567 , 0.49994335],\n       [0.50001734, 0.49998266],\n       [0.4999141 , 0.5000859 ],\n       [0.49987656, 0.50012344],\n       [0.5000711 , 0.4999289 ],\n       [0.50001353, 0.49998653],\n       [0.5003486 , 0.49965137],\n       [0.4999774 , 0.50002265],\n       [0.499837  , 0.500163  ],\n       [0.50011164, 0.49988836],\n       [0.5000038 , 0.49999624],\n       [0.49995923, 0.50004077],\n       [0.5000403 , 0.49995977],\n       [0.5000387 , 0.4999613 ],\n       [0.50009584, 0.4999042 ],\n       [0.49998823, 0.5000118 ],\n       [0.50010705, 0.49989286],\n       [0.4999913 , 0.5000087 ],\n       [0.49980834, 0.5001917 ],\n       [0.49982238, 0.5001777 ],\n       [0.4999937 , 0.5000063 ],\n       [0.50016516, 0.4998348 ],\n       [0.50018543, 0.4998146 ],\n       [0.5000063 , 0.4999937 ],\n       [0.4999376 , 0.5000624 ],\n       [0.49987507, 0.500125  ],\n       [0.49999756, 0.5000025 ],\n       [0.50009733, 0.49990264],\n       [0.50016856, 0.49983147],\n       [0.49987876, 0.5001212 ],\n       [0.5002715 , 0.4997285 ],\n       [0.50017935, 0.4998207 ],\n       [0.49993002, 0.50007   ],\n       [0.5000153 , 0.49998474],\n       [0.5000797 , 0.49992034],\n       [0.5001196 , 0.49988034],\n       [0.5000341 , 0.49996597],\n       [0.49986777, 0.50013226],\n       [0.5000014 , 0.49999863],\n       [0.49998584, 0.5000142 ]], dtype=float32)"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = model.predict(x_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# as softmax activation function output layer it return probability of output\n",
    "# discrete output is generated from that"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "y_pred_final=[]\n",
    "for items in y_pred:\n",
    "    if items[0]<items[1]:\n",
    "        y_pred_final.append(1)\n",
    "    else:\n",
    "        y_pred_final.append(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[51, 37],\n       [49, 53]])"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.array(y_pred_final)\n",
    "confusion_matrix(y_test, y_pred_final)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 540x540 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, y_pred_final)\n",
    "fig, ax = plt.subplots(figsize=(7.5, 7.5))\n",
    "ax.matshow(confusion_matrix, cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(confusion_matrix.shape[0]):\n",
    "    for j in range(confusion_matrix.shape[1]):\n",
    "        ax.text(x=j, y=i,s=confusion_matrix[i, j], va='center', ha='center', size='xx-large')\n",
    "\n",
    "plt.xlabel('Predictions', fontsize=18)\n",
    "plt.ylabel('Actuals', fontsize=18)\n",
    "plt.title('Confusion Matrix', fontsize=18)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.58      0.54        88\n",
      "           1       0.59      0.52      0.55       102\n",
      "\n",
      "    accuracy                           0.55       190\n",
      "   macro avg       0.55      0.55      0.55       190\n",
      "weighted avg       0.55      0.55      0.55       190\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred_final))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}